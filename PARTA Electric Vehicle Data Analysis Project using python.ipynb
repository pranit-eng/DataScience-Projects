{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eaaedc2-9897-487d-89f3-a8d709f9ac09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (50000, 2)\n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "Missing values:\n",
      "review       0\n",
      "sentiment    0\n",
      "dtype: int64\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Part A: IMDb Movie Review Sentiment Analysis\n",
    "# 1. Data Exploration and Preprocessing \n",
    "# Tasks:\n",
    "# Analyze the dataset: This includes checking for missing values, identifying class imbalances, and analyzing the length and structure of the reviews.\n",
    "# Data cleaning and preprocessing:\n",
    "# Remove stop words, punctuation, and special characters.\n",
    "# Tokenize the text (split the text into individual words).\n",
    "# Perform lemmatization and stemming to reduce words to their base form.\n",
    "# Use vectorization techniques like Bag-of-Words (BoW) and TF-IDF to convert text into numerical features.\n",
    "\n",
    "# Steps:\n",
    "# Load the dataset and check for trends:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the IMDb dataset \n",
    "df = pd.read_csv('imdb.csv')  \n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.head())  # First few rows\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Check the distribution of sentiments\n",
    "print(f\"Sentiment distribution:\\n{df['sentiment'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebafca52-2482-4b9c-9984-2836219af526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>one reviewer mentioned watching 1 oz episode y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>basically there family little boy jake think t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      cleaned_review  \n",
       "0  one reviewer mentioned watching 1 oz episode y...  \n",
       "1  wonderful little production br br filming tech...  \n",
       "2  thought wonderful way spend time hot summer we...  \n",
       "3  basically there family little boy jake think t...  \n",
       "4  petter matteis love time money visually stunni...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2\n",
    "# Text Preprocessing:\n",
    "# Remove stop words, punctuation, and special characters.\n",
    "# Tokenization: Split the reviews into individual words.\n",
    "# Lemmatization: Convert words to their base form.\n",
    "# Vectorization: Use TF-IDF or Bag-of-Words to convert text into a numerical format.\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Tokenization and lemmatization\n",
    "    tokens = word_tokenize(text.lower())  # Convert text to lowercase and tokenize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to each review\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned data\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a77d084d-b73d-4fb7-83ec-2bd9dddcba55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature matrix X: (50000, 5000)\n"
     ]
    }
   ],
   "source": [
    "# step 3\n",
    "# Vectorization:\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) is a common technique for transforming text into numerical \n",
    "# features that capture the importance of terms in a document.\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # Limit the features for simplicity\n",
    "X = vectorizer.fit_transform(df['cleaned_review']).toarray()\n",
    "y = df['sentiment'].map({'positive': 1, 'negative': 0})  # Convert sentiment to binary (1=positive, 0=negative)\n",
    "\n",
    "# Check the shape of the transformed features\n",
    "print(f\"Shape of feature matrix X: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd1d7821-954d-497c-950f-dd77906140d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of final feature matrix: (50000, 5003)\n"
     ]
    }
   ],
   "source": [
    "# Task 2. Feature Engineering \n",
    "# Tasks:\n",
    "# Feature extraction: Besides TF-IDF, we can extract additional textual features such as word count, character count, and average word length.\n",
    "\n",
    "# Steps:1 \n",
    "# Extracting textual features:\n",
    "\n",
    "# Additional features: word count, character count, average word length\n",
    "df['word_count'] = df['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "df['char_count'] = df['cleaned_review'].apply(lambda x: len(x.replace(\" \", \"\")))  # Remove spaces for char count\n",
    "df['avg_word_length'] = df['char_count'] / df['word_count']\n",
    "\n",
    "# Combine these features with the TF-IDF features\n",
    "X_additional = df[['word_count', 'char_count', 'avg_word_length']].values\n",
    "\n",
    "# Concatenate the features\n",
    "import numpy as np\n",
    "X_final = np.hstack((X, X_additional))  # Combine TF-IDF features and additional features\n",
    "\n",
    "print(f\"Shape of final feature matrix: {X_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cf373-97d5-46f5-bac7-d4cd69cacad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.8632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      4961\n",
      "           1       0.86      0.88      0.87      5039\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n",
      "Naive Bayes:\n",
      "Accuracy: 0.8246\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82      4961\n",
      "           1       0.83      0.83      0.83      5039\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Model Development \n",
    "# Tasks:\n",
    "# Build and train classification models: Experiment with various classification algorithms such as:\n",
    "# Logistic Regression\n",
    "# Naive Bayes\n",
    "# Support Vector Machine (SVM)\n",
    "# Random Forest\n",
    "# Neural Networks (e.g., LSTM, BERT)\n",
    "\n",
    "# step 1 Steps:\n",
    "# Train models using different algorithms:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "# Logistic Regression\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Logistic Regression model\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, lr_pred)}\")\n",
    "print(classification_report(y_test, lr_pred))\n",
    "\n",
    "# Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_pred = nb_model.predict(X_test)\n",
    "\n",
    "# Evaluate Naive Bayes model\n",
    "print(\"Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, nb_pred)}\")\n",
    "print(classification_report(y_test, nb_pred))\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate SVM model\n",
    "print(\"Support Vector Machine:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, svm_pred)}\")\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "print(\"Random Forest:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, rf_pred)}\")\n",
    "print(classification_report(y_test, rf_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14af47-f90e-43eb-983b-bf20ba3c893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 4 4. Model Evaluation \n",
    "# Tasks:\n",
    "# Evaluate the model’s performance using accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion Matrix for Logistic Regression as an example\n",
    "conf_matrix = confusion_matrix(y_test, lr_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "# Displaying classification report for better metrics visualization\n",
    "print(\"Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, lr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fbcda-fa2c-49e5-aa5e-cb31bce00473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
